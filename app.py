from importlib.resources import path
import streamlit as st 
import pandas as pd
import snscrape.modules.twitter as sntwitter
import matplotlib.pyplot as plt
from st_aggrid import AgGrid
from st_aggrid.grid_options_builder import GridOptionsBuilder
from st_aggrid import GridUpdateMode, DataReturnMode
from transformers import pipeline
from nltk.corpus import stopwords
from wordcloud import WordCloud, STOPWORDS
import PIL
from PIL import Image
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
import qalsadi.lemmatizer
import nltk
nltk.download('stopwords')
nltk.download('punkt')
import re
import emoji
import string
import numpy as np
import joblib as jb
from nltk.tokenize import word_tokenize
from sklearn.linear_model import LogisticRegression

vectorizer=TfidfVectorizer(max_features=1000,ngram_range=(1, 2))

def arabic_trained_model():
    preprocessing_data=pd.read_excel("final_preprocessing_dataset.xlsx")
    preprocessing_data.dropna(subset=['final_text_lemmatizer'], how='any', inplace=True)

    unigramdataGet= vectorizer.fit_transform(preprocessing_data['final_text_lemmatizer'].astype('str'))
    unigramdataGet = unigramdataGet.toarray()
    vocab = vectorizer.get_feature_names()
    unigramdata_features=pd.DataFrame(np.round(unigramdataGet, 1), columns=vocab)
    unigramdata_features[unigramdata_features>0] = 1

    Y=preprocessing_data.rating
    arabic_train_model = LogisticRegression().fit(unigramdata_features, Y)
    return arabic_train_model

    
train_model=arabic_trained_model()



def getX(df):
    arabic_punctuations = '''`Ã·Ã—Ø›<>_()*&^%][Ù€ØŒ/:"ØŸ.,'{}~Â¦+|!â€â€¦â€œâ€“Ù€'''
    english_punctuations = string.punctuation
    punctuations_list = arabic_punctuations + english_punctuations
    stop_words = list(set(stopwords.words('arabic')))
    print(stop_words)
    stop_words.remove('Ù„Ø§')
    stop_words.remove('Ù„ÙƒÙ†')
    stop_words.remove('ÙˆÙ„ÙƒÙ†')
    stop_words.remove('ÙˆØ§Ùˆ')
    stop_words.remove('Ø£Ø·Ø¹Ù…')
    stop_words.remove('Ø£Ù')
    def remove_diacritics(text):
        arabic_diacritics = re.compile(""" Ù‘    | # Tashdid
                             Ù    | # Fatha
                             Ù‹    | # Tanwin Fath
                             Ù    | # Damma
                             ÙŒ    | # Tanwin Damm
                             Ù    | # Kasra
                             Ù    | # Tanwin Kasr
                             Ù’    | # Sukun
                             Ù€     # Tatwil/Kashida
                         """, re.VERBOSE)
        text = re.sub(arabic_diacritics, '', str(text))
        return text

    def remove_emoji(text):
        regrex_pattern = re.compile(pattern = "["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               "]+", flags = re.UNICODE)
        return regrex_pattern.sub(r'',text)

    def clean_text(text):
        text = "".join([word for word in text if word not in string.punctuation])
        text = remove_emoji(text)
        text = remove_diacritics(text)
        tokens = word_tokenize(text)
        text = ' '.join([word for word in tokens if word not in stop_words])
        return text

    df.review_description= df.review_description.astype(str)
    df['cleaned_text'] = df.review_description.apply(clean_text)

    df.dropna(inplace=True)
    df.drop_duplicates(subset=['cleaned_text'],inplace=True)


    lemmer = qalsadi.lemmatizer.Lemmatizer()
    df['final_text'] = df.cleaned_text.apply(lambda x:lemmer.lemmatize_text(x))

    def convert_list_to_str(data):
        data = str(data)
        data = data.replace("'",'')
        data = data.replace(',','')                                                                                     
        data = data.replace('[','')
        data = data.replace(']','')

        return data

    df['final_text'] = df.final_text.apply(convert_list_to_str)
    df.to_excel('final_lemmatization.xlsx')

    text_features=vectorizer.transform(df["final_text"])
    my_array=text_features.toarray()
    X=pd.DataFrame(my_array,columns=vectorizer.get_feature_names())
    return X

def setup_model():  
    """
    Setup Model
    """
    # this will download 2 GB
    nlp = pipeline("sentiment-analysis", model='XLM-R-L-ARABIC-SENT')
    return nlp

def get_tweets(query,limit):
    """
    Get Tweets
    """
    #query = '"IBM" min_replies:10 min_faves:500 min_retweets:10 lang:ar'
    tweets = []
    pbar = st.progress(0)
    latest_iteration = st.empty()
    for tweet in sntwitter.TwitterSearchScraper(query).get_items():
        latest_iteration.text(f'{int(len(tweets)/limit*100)}% Done')
        pbar.progress(int(len(tweets)/limit*100))
        if len(tweets)==limit:
            break
        else:
            tweets.append(tweet.content)
    return tweets


# Fxn
def convert_to_df(sentiment):
    """
    Convert to df
    """
    label2 = sentiment[1] if 1 in sentiment.index else 0
    label1 = sentiment[-1] if -1 in sentiment.index else 0
    label0 = sentiment[0] if 0 in sentiment.index else 0
    sentiment_dict = {'Positive':label2,'Neutral':label0,'Negative':label1}
    sentiment_df = pd.DataFrame(sentiment_dict.items(),columns=['Sentiment','Count'])
    return sentiment_df

emojis = {
    "ğŸ™‚":"ÙŠØ¨ØªØ³Ù…",
    "ğŸ˜‚":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ’”":"Ù‚Ù„Ø¨ Ø­Ø²ÙŠÙ†",
    "ğŸ™‚":"ÙŠØ¨ØªØ³Ù…",
    "â¤ï¸":"Ø­Ø¨",
    "â¤":"Ø­Ø¨",
    "ğŸ˜":"Ø­Ø¨",
    "ğŸ˜­":"ÙŠØ¨ÙƒÙŠ",
    "ğŸ˜¢":"Ø­Ø²Ù†",
    "ğŸ˜”":"Ø­Ø²Ù†",
    "â™¥":"Ø­Ø¨",
    "ğŸ’œ":"Ø­Ø¨",
    "ğŸ˜…":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ™":"Ø­Ø²ÙŠÙ†",
    "ğŸ’•":"Ø­Ø¨",
    "ğŸ’™":"Ø­Ø¨",
    "ğŸ˜":"Ø­Ø²ÙŠÙ†",
    "ğŸ˜Š":"Ø³Ø¹Ø§Ø¯Ø©",
    "ğŸ‘":"ÙŠØµÙÙ‚",
    "ğŸ‘Œ":"Ø§Ø­Ø³Ù†Øª",
    "ğŸ˜´":"ÙŠÙ†Ø§Ù…",
    "ğŸ˜€":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ˜Œ":"Ø­Ø²ÙŠÙ†",
    "ğŸŒ¹":"ÙˆØ±Ø¯Ø©",
    "ğŸ™ˆ":"Ø­Ø¨",
    "ğŸ˜„":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ˜":"Ù…Ø­Ø§ÙŠØ¯",
    "âœŒ":"Ù…Ù†ØªØµØ±",
    "âœ¨":"Ù†Ø¬Ù…Ù‡",
    "ğŸ¤”":"ØªÙÙƒÙŠØ±",
    "ğŸ˜":"ÙŠØ³ØªÙ‡Ø²Ø¡",
    "ğŸ˜’":"ÙŠØ³ØªÙ‡Ø²Ø¡",
    "ğŸ™„":"Ù…Ù„Ù„",
    "ğŸ˜•":"Ø¹ØµØ¨ÙŠØ©",
    "ğŸ˜ƒ":"ÙŠØ¶Ø­Ùƒ",
    "ğŸŒ¸":"ÙˆØ±Ø¯Ø©",
    "ğŸ˜“":"Ø­Ø²Ù†",
    "ğŸ’":"Ø­Ø¨",
    "ğŸ’—":"Ø­Ø¨",
    "ğŸ˜‘":"Ù…Ù†Ø²Ø¹Ø¬",
    "ğŸ’­":"ØªÙÙƒÙŠØ±",
    "ğŸ˜":"Ø«Ù‚Ø©",
    "ğŸ’›":"Ø­Ø¨",
    "ğŸ˜©":"Ø­Ø²ÙŠÙ†",
    "ğŸ’ª":"Ø¹Ø¶Ù„Ø§Øª",
    "ğŸ‘":"Ù…ÙˆØ§ÙÙ‚",
    "ğŸ™ğŸ»":"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨",
    "ğŸ˜³":"Ù…ØµØ¯ÙˆÙ…",
    "ğŸ‘ğŸ¼":"ØªØµÙÙŠÙ‚",
    "ğŸ¶":"Ù…ÙˆØ³ÙŠÙ‚ÙŠ",
    "ğŸŒš":"ØµÙ…Øª",
    "ğŸ’š":"Ø­Ø¨",
    "ğŸ™":"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨",
    "ğŸ’˜":"Ø­Ø¨",
    "ğŸƒ":"Ø³Ù„Ø§Ù…",
    "â˜º":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ¸":"Ø¶ÙØ¯Ø¹",
    "ğŸ˜¶":"Ù…ØµØ¯ÙˆÙ…",
    "âœŒï¸":"Ù…Ø±Ø­",
    "âœ‹ğŸ»":"ØªÙˆÙ‚Ù",
    "ğŸ˜‰":"ØºÙ…Ø²Ø©",
    "ğŸŒ·":"Ø­Ø¨",
    "ğŸ™ƒ":"Ù…Ø¨ØªØ³Ù…",
    "ğŸ˜«":"Ø­Ø²ÙŠÙ†",
    "ğŸ˜¨":"Ù…ØµØ¯ÙˆÙ…",
    "ğŸ¼ ":"Ù…ÙˆØ³ÙŠÙ‚ÙŠ",
    "ğŸ":"Ù…Ø±Ø­",
    "ğŸ‚":"Ù…Ø±Ø­",
    "ğŸ’Ÿ":"Ø­Ø¨",
    "ğŸ˜ª":"Ø­Ø²Ù†",
    "ğŸ˜†":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ˜£":"Ø§Ø³ØªÙŠØ§Ø¡",
    "â˜ºï¸":"Ø­Ø¨",
    "ğŸ˜±":"ÙƒØ§Ø±Ø«Ø©",
    "ğŸ˜":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ˜–":"Ø§Ø³ØªÙŠØ§Ø¡",
    "ğŸƒğŸ¼":"ÙŠØ¬Ø±ÙŠ",
    "ğŸ˜¡":"ØºØ¶Ø¨",
    "ğŸš¶":"ÙŠØ³ÙŠØ±",
    "ğŸ¤•":"Ù…Ø±Ø¶",
    "â€¼ï¸":"ØªØ¹Ø¬Ø¨",
    "ğŸ•Š":"Ø·Ø§Ø¦Ø±",
    "ğŸ‘ŒğŸ»":"Ø§Ø­Ø³Ù†Øª",
    "â£":"Ø­Ø¨",
    "ğŸ™Š":"Ù…ØµØ¯ÙˆÙ…",
    "ğŸ’ƒ":"Ø³Ø¹Ø§Ø¯Ø© Ù…Ø±Ø­",
    "ğŸ’ƒğŸ¼":"Ø³Ø¹Ø§Ø¯Ø© Ù…Ø±Ø­",
    "ğŸ˜œ":"Ù…Ø±Ø­",
    "ğŸ‘Š":"Ø¶Ø±Ø¨Ø©",
    "ğŸ˜Ÿ":"Ø§Ø³ØªÙŠØ§Ø¡",
    "ğŸ’–":"Ø­Ø¨",
    "ğŸ˜¥":"Ø­Ø²Ù†",
    "ğŸ»":"Ù…ÙˆØ³ÙŠÙ‚ÙŠ",
    "âœ’":"ÙŠÙƒØªØ¨",
    "ğŸš¶ğŸ»":"ÙŠØ³ÙŠØ±",
    "ğŸ’":"Ø§Ù„Ù…Ø§Ø¸",
    "ğŸ˜·":"ÙˆØ¨Ø§Ø¡ Ù…Ø±Ø¶",
    "â˜":"ÙˆØ§Ø­Ø¯",
    "ğŸš¬":"ØªØ¯Ø®ÙŠÙ†",
    "ğŸ’" : "ÙˆØ±Ø¯",
    "ğŸŒ" : "Ø´Ù…Ø³",
    "ğŸ‘†" : "Ø§Ù„Ø§ÙˆÙ„",
    "âš ï¸" :"ØªØ­Ø°ÙŠØ±",
    "ğŸ¤—" : "Ø§Ø­ØªÙˆØ§Ø¡",
    "âœ–ï¸": "ØºÙ„Ø·",
    "ğŸ“"  : "Ù…ÙƒØ§Ù†",
    "ğŸ‘¸" : "Ù…Ù„ÙƒÙ‡",
    "ğŸ‘‘" : "ØªØ§Ø¬",
    "âœ”ï¸" : "ØµØ­",
    "ğŸ’Œ": "Ù‚Ù„Ø¨",
    "ğŸ˜²" : "Ù…Ù†Ø¯Ù‡Ø´",
    "ğŸ’¦": "Ù…Ø§Ø¡",
    "ğŸš«" : "Ø®Ø·Ø§",
    "ğŸ‘ğŸ»" : "Ø¨Ø±Ø§ÙÙˆ",
    "ğŸŠ" :"ÙŠØ³Ø¨Ø­",
    "ğŸ‘ğŸ»": "ØªÙ…Ø§Ù…",
    "â­•ï¸" :"Ø¯Ø§Ø¦Ø±Ù‡ ÙƒØ¨ÙŠØ±Ù‡",
    "ğŸ·" : "Ø³Ø§ÙƒØ³ÙÙˆÙ†",
    "ğŸ‘‹": "ØªÙ„ÙˆÙŠØ­ Ø¨Ø§Ù„ÙŠØ¯",
    "âœŒğŸ¼": "Ø¹Ù„Ø§Ù…Ù‡ Ø§Ù„Ù†ØµØ±",
    "ğŸŒ":"Ù…Ø¨ØªØ³Ù…",
    "â¿"  : "Ø¹Ù‚Ø¯Ù‡ Ù…Ø²Ø¯ÙˆØ¬Ù‡",
    "ğŸ’ªğŸ¼" : "Ù‚ÙˆÙŠ",
    "ğŸ“©":  "ØªÙˆØ§ØµÙ„ Ù…Ø¹ÙŠ",
    "â˜•ï¸": "Ù‚Ù‡ÙˆÙ‡",
    "ğŸ˜§" : "Ù‚Ù„Ù‚ Ùˆ ØµØ¯Ù…Ø©",
    "ğŸ—¨": "Ø±Ø³Ø§Ù„Ø©",   
    "â—ï¸" :"ØªØ¹Ø¬Ø¨",
    "ğŸ™†ğŸ»": "Ø§Ø´Ø§Ø±Ù‡ Ù…ÙˆØ§ÙÙ‚Ù‡",
    "ğŸ‘¯" :"Ø§Ø®ÙˆØ§Øª",
    "Â©" :  "Ø±Ù…Ø²",
    "ğŸ‘µğŸ½" :"Ø³ÙŠØ¯Ù‡ Ø¹Ø¬ÙˆØ²Ù‡",
    "ğŸ£": "ÙƒØªÙƒÙˆØª",  
    "ğŸ™Œ": "ØªØ´Ø¬ÙŠØ¹",
    "ğŸ™‡": "Ø´Ø®Øµ ÙŠÙ†Ø­Ù†ÙŠ",
    "ğŸ‘ğŸ½":"Ø§ÙŠØ¯ÙŠ Ù…ÙØªÙˆØ­Ù‡",    
    "ğŸ‘ŒğŸ½": "Ø¨Ø§Ù„Ø¸Ø¨Ø·",
    "â‰ï¸" : "Ø§Ø³ØªÙ†ÙƒØ§Ø±",
    "âš½ï¸": "ÙƒÙˆØ±Ù‡",
    "ğŸ•¶" :"Ø­Ø¨",
    "ğŸˆ" :"Ø¨Ø§Ù„ÙˆÙ†",    
    "ğŸ€":    "ÙˆØ±Ø¯Ù‡",
    "ğŸ’µ":  "ÙÙ„ÙˆØ³",   
    "ğŸ˜‹":  "Ø¬Ø§Ø¦Ø¹",
    "ğŸ˜›":  "ÙŠØºÙŠØ¸",
    "ğŸ˜ ":  "ØºØ§Ø¶Ø¨",
    "âœğŸ»":  "ÙŠÙƒØªØ¨",
    "ğŸŒ¾":  "Ø§Ø±Ø²",
    "ğŸ‘£":  "Ø§Ø«Ø± Ù‚Ø¯Ù…ÙŠÙ†",
    "âŒ":"Ø±ÙØ¶",
    "ğŸŸ":"Ø·Ø¹Ø§Ù…",
    "ğŸ‘¬":"ØµØ¯Ø§Ù‚Ø©",
    "ğŸ°":"Ø§Ø±Ù†Ø¨",
    "â˜‚":"Ù…Ø·Ø±",
    "âšœ":"Ù…Ù…Ù„ÙƒØ© ÙØ±Ù†Ø³Ø§",
    "ğŸ‘":"Ø®Ø±ÙˆÙ",
    "ğŸ—£":"ØµÙˆØª Ù…Ø±ØªÙØ¹",
    "ğŸ‘ŒğŸ¼":"Ø§Ø­Ø³Ù†Øª",
    "â˜˜":"Ù…Ø±Ø­",
    "ğŸ˜®":"ØµØ¯Ù…Ø©",
    "ğŸ˜¦":"Ù‚Ù„Ù‚",
    "â­•":"Ø§Ù„Ø­Ù‚",
    "âœï¸":"Ù‚Ù„Ù…",
    "â„¹":"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª",
    "ğŸ™ğŸ»":"Ø±ÙØ¶",
    "âšªï¸":"Ù†Ø¶Ø§Ø±Ø© Ù†Ù‚Ø§Ø¡",
    "ğŸ¤":"Ø­Ø²Ù†",
    "ğŸ’«":"Ù…Ø±Ø­",
    "ğŸ’":"Ø­Ø¨",
    "ğŸ”":"Ø·Ø¹Ø§Ù…",
    "â¤ï¸":"Ø­Ø¨",
    "âœˆï¸":"Ø³ÙØ±",
    "ğŸƒğŸ»â€â™€ï¸":"ÙŠØ³ÙŠØ±",
    "ğŸ³":"Ø°ÙƒØ±",
    "ğŸ¤":"Ù…Ø§ÙŠÙƒ ØºÙ†Ø§Ø¡",
    "ğŸ¾":"ÙƒØ±Ù‡",
    "ğŸ”":"Ø¯Ø¬Ø§Ø¬Ø©",
    "ğŸ™‹":"Ø³Ø¤Ø§Ù„",
    "ğŸ“®":"Ø¨Ø­Ø±",
    "ğŸ’‰":"Ø¯ÙˆØ§Ø¡",
    "ğŸ™ğŸ¼":"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨",
    "ğŸ’‚ğŸ¿ ":"Ø­Ø§Ø±Ø³",
    "ğŸ¬":"Ø³ÙŠÙ†Ù…Ø§",
    "â™¦ï¸":"Ù…Ø±Ø­",
    "ğŸ’¡":"Ù‚ÙƒØ±Ø©",
    "â€¼":"ØªØ¹Ø¬Ø¨",
    "ğŸ‘¼":"Ø·ÙÙ„",
    "ğŸ”‘":"Ù…ÙØªØ§Ø­",
    "â™¥ï¸":"Ø­Ø¨",
    "ğŸ•‹":"ÙƒØ¹Ø¨Ø©",
    "ğŸ“":"Ø¯Ø¬Ø§Ø¬Ø©",
    "ğŸ’©":"Ù…Ø¹ØªØ±Ø¶",
    "ğŸ‘½":"ÙØ¶Ø§Ø¦ÙŠ",
    "â˜”ï¸":"Ù…Ø·Ø±",
    "ğŸ·":"Ø¹ØµÙŠØ±",
    "ğŸŒŸ":"Ù†Ø¬Ù…Ø©",
    "â˜ï¸":"Ø³Ø­Ø¨",
    "ğŸ‘ƒ":"Ù…Ø¹ØªØ±Ø¶",
    "ğŸŒº":"Ù…Ø±Ø­",
    "ğŸ”ª":"Ø³ÙƒÙŠÙ†Ø©",
    "â™¨":"Ø³Ø®ÙˆÙ†ÙŠØ©",
    "ğŸ‘ŠğŸ¼":"Ø¶Ø±Ø¨",
    "âœ":"Ù‚Ù„Ù…",
    "ğŸš¶ğŸ¾â€â™€ï¸":"ÙŠØ³ÙŠØ±",
    "ğŸ‘Š":"Ø¶Ø±Ø¨Ø©",
    "â—¾ï¸":"ÙˆÙ‚Ù",
    "ğŸ˜š":"Ø­Ø¨",
    "ğŸ”¸":"Ù…Ø±Ø­",
    "ğŸ‘ğŸ»":"Ù„Ø§ ÙŠØ¹Ø¬Ø¨Ù†ÙŠ",
    "ğŸ‘ŠğŸ½":"Ø¶Ø±Ø¨Ø©",
    "ğŸ˜™":"Ø­Ø¨",
    "ğŸ¥":"ØªØµÙˆÙŠØ±",
    "ğŸ‘‰":"Ø¬Ø°Ø¨ Ø§Ù†ØªØ¨Ø§Ù‡",
    "ğŸ‘ğŸ½":"ÙŠØµÙÙ‚",
    "ğŸ’ªğŸ»":"Ø¹Ø¶Ù„Ø§Øª",
    "ğŸ´":"Ø§Ø³ÙˆØ¯",
    "ğŸ”¥":"Ø­Ø±ÙŠÙ‚",  
    "ğŸ˜¬":"Ø¹Ø¯Ù… Ø§Ù„Ø±Ø§Ø­Ø©",   
    "ğŸ‘ŠğŸ¿":"ÙŠØ¶Ø±Ø¨",    
    "ğŸŒ¿":"ÙˆØ±Ù‚Ù‡ Ø´Ø¬Ø±Ù‡",     
    "âœ‹ğŸ¼":"ÙƒÙ Ø§ÙŠØ¯",    
    "ğŸ‘":"Ø§ÙŠØ¯ÙŠ Ù…ÙØªÙˆØ­Ù‡",      
    "â˜ ï¸":"ÙˆØ¬Ù‡ Ù…Ø±Ø¹Ø¨",     
    "ğŸ‰":"ÙŠÙ‡Ù†Ø¦",      
    "ğŸ”•" :"ØµØ§Ù…Øª",
    "ğŸ˜¿":"ÙˆØ¬Ù‡ Ø­Ø²ÙŠÙ†",      
    "â˜¹ï¸":"ÙˆØ¬Ù‡ ÙŠØ§Ø¦Ø³",     
    "ğŸ˜˜" :"Ø­Ø¨",     
    "ğŸ˜°" :"Ø®ÙˆÙ Ùˆ Ø­Ø²Ù†",
    "ğŸŒ¼":"ÙˆØ±Ø¯Ù‡",      
    "ğŸ’‹":  "Ø¨ÙˆØ³Ù‡",
    "ğŸ‘‡":"Ù„Ø§Ø³ÙÙ„",     
    "â£ï¸":"Ø­Ø¨",     
    "ğŸ§":"Ø³Ù…Ø§Ø¹Ø§Øª",
    "ğŸ“":"ÙŠÙƒØªØ¨",      
    "ğŸ˜‡":"Ø¯Ø§ÙŠØ®",      
    "ğŸ˜ˆ":"Ø±Ø¹Ø¨",      
    "ğŸƒ":"ÙŠØ¬Ø±ÙŠ",      
    "âœŒğŸ»":"Ø¹Ù„Ø§Ù…Ù‡ Ø§Ù„Ù†ØµØ±",    
    "ğŸ”«":"ÙŠØ¶Ø±Ø¨",      
    "â—ï¸":"ØªØ¹Ø¬Ø¨",
    "ğŸ‘":"ØºÙŠØ± Ù…ÙˆØ§ÙÙ‚",      
    "ğŸ”":"Ù‚ÙÙ„",      
    "ğŸ‘ˆ":"Ù„Ù„ÙŠÙ…ÙŠÙ†",
    "â„¢":"Ø±Ù…Ø²",    
    "ğŸš¶ğŸ½":"ÙŠØªÙ…Ø´ÙŠ",    
    "ğŸ˜¯":"Ù…ØªÙØ§Ø¬Ø£",  
    "âœŠ":"ÙŠØ¯ Ù…ØºÙ„Ù‚Ù‡",    
    "ğŸ˜»":"Ø§Ø¹Ø¬Ø§Ø¨",    
    "ğŸ™‰" :"Ù‚Ø±Ø¯",    
    "ğŸ‘§":"Ø·ÙÙ„Ù‡ ØµØºÙŠØ±Ù‡",     
    "ğŸ”´":"Ø¯Ø§Ø¦Ø±Ù‡ Ø­Ù…Ø±Ø§Ø¡",      
    "ğŸ’ªğŸ½":"Ù‚ÙˆÙ‡",     
    "ğŸ’¤":"ÙŠÙ†Ø§Ù…",     
    "ğŸ‘€":"ÙŠÙ†Ø¸Ø±",     
    "âœğŸ»":"ÙŠÙƒØªØ¨",  
    "â„ï¸":"ØªÙ„Ø¬",
    "ğŸ’€":"Ø±Ø¹Ø¨",   
    "ğŸ˜¤":"ÙˆØ¬Ù‡ Ø¹Ø§Ø¨Ø³",      
    "ğŸ–‹":"Ù‚Ù„Ù…",      
    "ğŸ©":"ÙƒØ§Ø¨",      
    "â˜•ï¸":"Ù‚Ù‡ÙˆÙ‡",     
    "ğŸ˜¹":"Ø¶Ø­Ùƒ",     
    "ğŸ’“":"Ø­Ø¨",      
    "â˜„ï¸ ":"Ù†Ø§Ø±",     
    "ğŸ‘»":"Ø±Ø¹Ø¨",
    "â":"Ø®Ø·Ø¡",
    "ğŸ¤®":"Ø­Ø²Ù†",
    'ğŸ»':"Ø§Ø­Ù…Ø±"
}

emoticons_to_emoji = {
    ":)" : "ğŸ™‚",
    ":(" : "ğŸ™",
    "xD" : "ğŸ˜†",
    ":=(": "ğŸ˜­",
    ":'(": "ğŸ˜¢",
    ":'â€‘(": "ğŸ˜¢",
    "XD" : "ğŸ˜‚",
    ":D" : "ğŸ™‚",
    "â™¬" : "Ù…ÙˆØ³ÙŠÙ‚ÙŠ",
    "â™¡" : "â¤",
    "â˜»"  : "ğŸ™‚",
}

def data_preprocessing(data):
 
    X = getX(data)
    sentiment = train_model.predict(X)
    return sentiment

def get_sentiments(tweets,opt):
    """
    Get sentiments
    """
    if opt =='Arabic':
        sentiments = []
        pbar = st.progress(0)
        latest_iteration = st.empty()
        tweets=pd.DataFrame(tweets,columns=['review_description'])
        sentiments=data_preprocessing(tweets)
        x=pd.DataFrame(sentiments)
        x.to_excel('counts.xlsx')
    else:
        nlp = setup_model()
        sentiments = []
        pbar = st.progress(0)
        latest_iteration = st.empty()
        for i,tweet in enumerate(tweets):
            latest_iteration.text(f'{int((i+1)/len(tweets)*100)}% Done')
            pbar.progress(int((i+1)/len(tweets)*100))
            sentiments.append(nlp(tweet)[0]['label'])
    return sentiments

def main():
    """
    Main
    """
    st.set_page_config(layout="wide",initial_sidebar_state="collapsed")
    col1,col2,col3 = st.columns((1,2,1))
    image = Image.open('banquemisr.png')
    #smileys = Image.open('smileys.png')
    with col3:
        st.image(image)
    with col1:
        st.title("Company Based Sentiment")
        st.subheader("Made by Fahd Seddik")
    with col2:
        st.title("")
    menu = ["Home","About"]
    choice = st.sidebar.selectbox("Menu",menu)
    if choice == "Home":
        with col1:
            st.subheader("Home")
            
            temp = st.slider("Choose sample size",min_value=50,max_value=10000)
            opt = st.selectbox('Language',pd.Series(['Arabic','English']))
            likes = st.slider("Minimum number of likes on tweet",min_value=0,max_value=5000)
            retweets = st.slider("Minimum number of retweets on tweet",min_value=0,max_value=500)
            with st.form(key='nlpForm'):
                raw_text = st.text_area("Enter company name here")
                submit_button = st.form_submit_button(label='Analyze')       
        # layout
        if submit_button:
            with col2:
                st.success(f'Selected Language: {opt}',)
                if opt=='Arabic':
                    query = raw_text + ' lang:ar'
                else:
                    query = raw_text + ' lang:en'
                query += f' min_faves:{likes} min_retweets:{retweets}'
                st.write('Retreiving Tweets...')
                tweets = get_tweets(query,temp)
                if(len(tweets)==temp):
                    st.success(f'Found {len(tweets)}/{temp}')
                else:
                    st.error(f'Only Found {len(tweets)}/{temp}. Try changing min_likes, min_retweets')
                st.write('Loading Model...')
                #nlp = setup_model()
                st.success('Loaded Model')
                st.write('Analyzing Sentiments...')
                sentiments = get_sentiments(tweets,opt)
                st.success('DONE')
                st.subheader("Results of "+raw_text)
                counts = pd.Series(sentiments).value_counts()
                result_df = convert_to_df(counts)
                # Dataframe
                st.dataframe(result_df)

                fig1, ax1 = plt.subplots(figsize=(5,5))
                ax1.pie(result_df['Count'],labels=result_df['Sentiment'].values, autopct='%1.1f%%',
                        shadow=True, startangle=90,colors=['green','yellow','red'])
                ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
                st.pyplot(fig1)
                tweets_s = pd.Series(tweets,name='Tweet')
                sentiments_s = pd.Series(sentiments,name='Sentiment (pred)').replace(
                    {'LABEL_2':'Positive','LABEL_1':'Negative','LABEL_0':'Neutral'})
                all_df = pd.merge(left=tweets_s,right=sentiments_s,left_index=True,right_index=True)
                all_df.to_excel('tweets.xlsx')
            st.subheader("Tweets")
            gb = GridOptionsBuilder.from_dataframe(all_df)
            gb.configure_side_bar()
            grid_options = gb.build()
            AgGrid(
                all_df,
                gridOptions=grid_options,
                enable_enterprise_modules=True,
                update_mode=GridUpdateMode.MODEL_CHANGED,
                data_return_mode=DataReturnMode.FILTERED_AND_SORTED,
                fit_columns_on_grid_load=False,
            )
            with col2:
                st.subheader("Word Cloud")
                #Word Cloud
                if opt=='English':
                    words = " ".join(word for tweet in tweets for word in tweet.split())
                    st_en = set(stopwords.words('english'))
                    #st_ar = set(stopwords.words('arabic'))
                    st_en = st_en.union(STOPWORDS).union(set(['https','http','DM','dm','via','co']))
                    wordcloud = WordCloud(stopwords=st_en, background_color="white", width=800, height=400)
                    wordcloud.generate(words)
                    fig2, ax2 = plt.subplots(figsize=(5,5))
                    ax2.axis("off")
                    fig2.tight_layout(pad=0)
                    ax2.imshow(wordcloud, interpolation='bilinear')
                    st.pyplot(fig2)
                else:
                    st.error(f'WordCloud not available for Language = {opt}')

    else:
        st.subheader("About")
        st.write("This was made in order to have an idea about people's opinion on a certain company. The program scrapes twitter for tweets\
             that are about a certain company. The tweets are then fed into a model for sentiment analysis which is then used \
            to display useful information about each company and public opinion.")


if __name__ == '__main__':
	main()